\documentclass{article}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\setlength{\parindent}{0mm}

\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\N}[0]{\mathbb{N}}
\newcommand{\F}[0]{\mathcal{F}}
\newcommand{\E}[0]{\mathbb{E}}

\title{Datascience}
\author{Yannik Höll}
\date{\today}

\newtheorem{defin}{Definition}
\newtheorem{bem}{Bemerkung}
\newtheorem{lemma}{Lemma}
\newtheorem{definthm}{Definition \& Satz}
\newtheorem{thm}{Satz}

\begin{document}
\maketitle

\section{Inverse Faltung}

\subsection{Klassische Fouriertransformation und Faltung}

Für folgende Definitionen sei $n \in \mathbb{N} \setminus \{ 0 \}$.

\begin{defin}
    Seien $f,g \in L^1(\R^n)$. \\
    Dann ist die \textbf{Konvolution} von $f$ mit $g$ für $x \in \Omega$ definiert als:
    \begin{equation}
        (f \star g)(x) := \int_{\R^n} f(x - y) g(y) dy
    \end{equation}
\end{defin}

\begin{defin} Die \textbf{Fourier-Transformation} einer Funktion $f \in L^1(\R^n)$ ist definiert durch:
    \begin{equation}
        (\mathcal{F}f)(\xi) := \int_{\R^n} f(x) \cdot e^{-2\pi i x \cdot \xi} dx
    \end{equation}
    für $x, \xi \in \R^n$. Dabei ist $x \cdot \xi$ das Skalarprodukt im $\R^n$.
\end{defin}

Die Fouriertransformation ist wohldefiniert, weil $f \in L^1(\R^n)$.
\begin{proof}
    Sei $f \in L^1(\R^n)$, dann gilt:
    \begin{align*}
        \Bigg\lvert \int_{\R^n} f(x) \cdot e^{-2\pi i x \cdot \xi} dx \Bigg\rvert &\leq \int_{\R^n} \lvert f(x) \rvert \lvert e^{-2\pi i x \cdot \xi} \rvert dx \\
        &= \int_{\R^n} \lvert f(x) \rvert dx \\
        &= \lVert f \rVert_{L^1(\R^n)} < \infty
    \end{align*}
\end{proof}

\begin{defin}
    Die \textbf{Inverse Fouriertransformation} einer Funktion $f \in L^1(\R^n)$ ist definiert durch:
    \begin{equation}
        (\mathcal{F}f)(\xi) := \int_{\R^n} f(x) \cdot e^{2\pi i x \cdot \xi} dx
    \end{equation}
    für $x, \xi \in \R^n$. Dabei ist $x \cdot \xi$ das Skalarprodukt im $\R^n$.
\end{defin}
Der Nachweis der Wohldefiniertheit funktioniert genauso, wie bei der Fouriertransformation.

\begin{thm}
    \textbf{Faltungssatz} \\
    Seien $f,g \in L^1(\R^n)$ und $\mathcal{F}f \cdot \mathcal{F}g \in L^1(\R^n)$. Dann gilt:
    \begin{equation}
        \mathcal{F}(f \star g) = \mathcal{F}f \cdot \mathcal{F}g
    \end{equation}

    \begin{proof}
        Der Beweis ist abgeschlossen, wenn gezeigt wurde, dass $\F^{-1} (\F f \cdot \F g) = f \star g$. 
        
        Sei $F(\omega) := \F f(\omega)$ und $G(\omega) := \F g(\omega)$. Dann gilt: \\
        \begin{align*}
            \F^{-1} (F(\omega) G(\omega)) &= \int_{\R^n} F(\omega) G(\omega) \cdot e^{2\pi i \omega \cdot t} d\omega \\
            &= \int_{\R^n} \Bigg\lbrack \int_{\R^n} f(s) \cdot e^{-2\pi i s \cdot \omega} ds \Bigg\rbrack G(\omega) \cdot e^{2\pi i \omega \cdot} d\omega \\
            &= \int_{\R^n} \int_{\R^n} f(s) G(\omega) \cdot e^{2\pi i \omega \cdot (s - t)} ds d\omega
        \end{align*}
        Nach dem Satz von Fubini kann die Integrationsreihenfolge vertauscht werden:
        \begin{align*}
            \F^{-1} (F(\omega) G(\omega)) &= \int_{\R^n} \int_{\R^n} f(s) G(\omega) \cdot e^{2\pi i \omega \cdot (s - t)} d\omega ds \\
            &= \int_{\R^n} f(s) \Bigg \lbrack \int_{\R^n} G(\omega) \cdot e^{2\pi i \omega \cdot (t - s)} d\omega \Bigg \rbrack ds \\
            &= \int_{\R^n} f(s) g(t-s) ds = f \star g
        \end{align*}
    \end{proof}
\end{thm}

\subsection{Diskrete Fouriertransformation}

Mit Satz 1 ist es nun theoretisch möglich, die Faltung von zwei Funktionen umzukehren, unter der Vorraussetzung, 
dass eine der beiden Funktionen bekannt ist. Dafür kann man die Fouriertransformation wie folgt nutzen:

Seien $w: \R \to \R$ und $f, g \in L^1(\R)$, sodass gilt: $w = f \star g$. Dabei sind $w$ und $g$ bekannt und $f$ ist die gesuchte Funktion.
Wenn $w$ und $g$ hinreichund gute Eigenschaften haben, sodass die Integrale existieren und die Inverse Fouriertransformation existiert, dann kann
man folgenden Algorithmus nutzen, um $f$ zu berechnen:
\begin{enumerate}
    \item Berechne $\hat{w} := \mathcal{F}w$ und $\hat{g} := \mathcal{F}g$.
    \item Berechne $\hat{f} := \frac{\hat{w}}{\hat{g}}$.
    \item Berechne $f := \mathcal{F}^{-1}\hat{f}$.
\end{enumerate}

Durch die Berechnung im 2. Schritt wird sofort deutlich, dass $\hat{g}$ keine Nullstellen besitzen darf,
da ansonsten der Quotient nicht existiert. Zusätzlich muss man auch über Stetigkeitseigenschaften der Methode nachdenken,
da man Komponenten von hohen Frequenzen von $w$ potentiell stark aufgebläht werden 
und damit auch potentielle Messfehler verstärt werden können.

Für konkrete Berechnungen benötigt man die diskrete Fouriertransformation. 
Sei dafür $u \in C^0(\R)$ mit $supp(u) := \{x \in \R \: | \: u(x) \neq 0 \} \subseteq [-a, a], a > 0$. Diese Funktion wird an $N \in \mathbb{N}_{>0}$ äquidistanten Stellen betrachtet, 
die z.B. Messwerte darstellen können. Sei dafür $u_j = u(t_j), t_j := jh, h:= \frac{2a}{N}, j \in \{-\frac{N}{2}, \cdots, \frac{N}{2} - 1\}$.

Dann kann $u$ durch lineare B-Splines approximiert werden durch:
\begin{equation}
    B_2(t) := \begin{cases}
        t + 1, &-1 \leq t \leq 0, \\
        1 - t, &0 \leq t \leq 1, \\
        0, &{\rm sonst}.
    \end{cases} \\
\end{equation}
\begin{equation}
    u_N(t) := \sum\limits_{j=-N/2}^{N/2-1} u_j B_2 \left(\frac{t}{h} - j\right)
\end{equation}

Durch Fouriertransformation erhält man:
\begin{equation}
    \mathcal{F}(u_N)(y) := \left(\frac{{\rm sin}(\pi h y)}{\pi h y}\right) \cdot \underbrace{\left(h \sum\limits_{j=-N/2}^{N/2-1} u_j e^{-2\pi i jhy}\right)}_{:=U(y)}
\end{equation}

Wählt man $y := \frac{k}{2a}, \: k \in \mathbb{Z}$, dann sind die Stellen, an denen man die Funktion $u$ und die Transformierte $\mathcal{F}(u_N)$
auswertet äquidistant. Damit erhält man schlussendlich:

\begin{align}
    U_k = \frac{1}{N} \sum\limits_{j=-N/2}^{N/2-1} u_j e^{-2\pi ijk/N}
\iff u_j = \frac{1}{N} \sum\limits_{k=-N/2}^{N/2-1} U_k e^{2k\pi ijk/N}
\end{align}

Dabei sind die $U_k$ nun die diskreten Fourierkoeffizienten von $u$ zu den entsprechenden Stützstellen.

Diese diskrete Variante der Transformation kann nun verwendet werden, um die Inverse Faltung wie oben beschrieben für konkrete Messwerte zu berechnen. 
Es handelt sich allerdings um ein schlecht gestelltes Problem und man muss Regularisierungen anwenden, um die die schlechten Stetigkeitseigenschaften der Methode zu verbessern. 

\section{PLSR}

\subsection{Singulärwert-Zerlegung (SVD)}
\textbf{Notation}:
\begin{enumerate}
    \item $A \in \R^{m \times n}: \: \mathcal{N}(A) := \{x \in \R^n \: | \: Ax = 0\}$ ist der \textbf{Nullraum} der Matrix $A$
\end{enumerate}

Die Singulärwertzerlegung ist eine Verallgemeinerung der Eigenwertzerlegung einer Matrix, die auch auf nicht-quadratische, singuläre Matritzen angewandt werden kann.

\begin{definthm}
    Sei $A \in \R^{m \times n}$ eine reellwertige Matrix mit dem Rang $r$ $(m,n \in \mathbb{N}_{>0})$.

    Die \textbf{Singulärwertzerlegung} von $A$  hat folgende Gestalt:
    \begin{equation}
        A = U \Sigma V^T
    \end{equation}
    wobei $U \in \R^{m \times m}, \; \Sigma \in \R^{m \times n}, \; V \in \R^{m \times m}$.
    Die Spalten von $U$ nennt man die \textbf{Links-Singulärvektoren} und die von $V$ die \textbf{Rechts-Singulärvektoren} von $A$. $U, V$ sind orthogonale Matritzen.
    $\Sigma$ ist eine Diagonalmatrix deren Einträge $\sigma_i$ $(i \in \{1, \cdots, r\})$ die \textbf{Singulärwerte} von $A$ genannt werden.
    
    Die Matritzen erfüllen folgendes Gleichungssystem:
    \begin{align}
        &Mv_i = \sigma_i u_i, \:&&\forall i \in \{1, \cdots, r\} \\
        &Mv_i = 0, \: &&\forall i \in \{r + 1, \cdots, n\}
    \end{align}

    Diese Zerlegung existiert für jede beliebige Matrix $A \in \R^{m \times n}$.
\end{definthm}

\begin{proof}
    Betrachte
    \begin{align}
        A^TA = (V \Sigma^T U^T) (U \Sigma V^T) &= V \Sigma^T \Sigma V^T \\
        AA^T = (U \Sigma V^T) (V \Sigma^T U^T) &= U \Sigma \Sigma^T U^T
    \end{align}
    Diese beiden Matritzen sind symmetrische, quadratische Matritzen und haben somit
    nicht-negative Eigenwerte. Man kann also beide Matritzen in der Form $Q \Lambda Q^T$ darstellen.
    Dabei ist $V = Q$ für Gleichung (12) und $U = Q$ für Gleichung (13). Und somit sind $V$ bzw. $U$ die Eigenwerte
    von $A^TA$ bzw. $AA^T$ und insbesondere orthogonale Matritzen.

    \textbf{Konstruktion von $V$}: Wähle orthonormale Eigenvektoren $v_1, \cdots, v_r$ von $A^TA$.

    \textbf{Konstruktion von $\Sigma$}: Setze $\sigma_i = \sqrt{\lambda_i}, \: \forall i \in \{1, \cdots, r\}, \lambda_i \in \Lambda$.

    \textbf{Konstruktion von $U$}: Setze $u_i = \frac{Av_i}{\sigma_i}, \: \forall i \in \{1, \cdots, r\}$.
    
    \textbf{Behautung}: 
    \begin{enumerate}
        \item $u_i$ sind die Eigenvektoren zu den Eigenwerten $\lambda_i$ für $AA^T$.
        \item Die $u_i$ sind orthonormale Vektoren.
    \end{enumerate}
    \begin{equation*}
        AA^Tu_k = AA^T\left(\frac{Av_k}{\sigma_k}\right) = A\left(\frac{A^TAv_k}{\sigma_k}\right) = A\frac{\sigma_k^2v_k}{\sigma_k} = \sigma_k^2u_k
    \end{equation*}
    Somit ist 1. von der Behauptung gezeigt.

    \begin{equation*}
        u_j^Tu_k = \left(\frac{Av_j}{\sigma_j}\right)^T\left(\frac{Av_k}{\sigma_k}\right) = \frac{v_j^T(A^TAv_k)}{\sigma_j\sigma_k} = \frac{\sigma_k}{\sigma_j}v_j^Tv_k = \begin{cases}
            1, &\text{für } j = k,\\
            0, &\text{sonst}.
        \end{cases}
    \end{equation*}
    Das zeigt den 2. Teil der Behauptung.

    Nun müssen noch die letzen $n - r$ Vektoren für $V$ und die letzen $m - r$ Vektoren für $U$ gewählt werden.
    Man wählt eine Orthonormalbasis für den Nullraum von $A$ und diese bilden dann die fehlenden Vektoren für $V$ 
    und weiterhin wählt man eine Orthonormalbasis für den Nullraum von $A^T$ und diese bilden die fehlenden Vektoren für $U$.
    Die so gewählten Vektoren $v_i$ sind automatisch orthogonal zu den bereits gewählten $v_1, \cdots, v_r$ 
    und dasselbe gilt für die so gewählten $u_i$. 
\end{proof}

\begin{thm} \textbf{Satz von Eckard-Young}
    
    Sei $A \in \R^{m\times n}$ eine Rang $r$ Matrix. Weiterhin sei $A_k = \sum\limits_{i=1}^k u_i\sigma_iv_i^T$. 
    Wobei $\sigma_i$ die ersten $k$ Singulärwerte von A sind, wobei gilt $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r \geq 0$. Die $u_i, v_i$ sind die dazugehörigen Singulärvektoren. 
    Dann gilt für $\forall B \in \R^{m \times n}$ mit Rang $k$:
    \begin{equation}
        \lVert A - B \rVert \geq \lVert A - A_k \rVert
    \end{equation}
\end{thm}

Diese Formulierung gilt für eine ganze Reihe von verschiedenen Operatornormen.
Nachfolgend ist exemplarisch der Beweis des Satzes von Eckard-Young für die Spektralnorm.
Dieser illustriert auf natürliche Weise die Aussage das Satzes.

\begin{defin}
    Sei $A \in \R^{m \times n}$. Dann ist die \textbf{Spektralnorm} von $A$ definiert durch:
    \begin{equation}
        \lVert A \rVert_2 \: := \: \max\limits_{x \neq 0} \frac{\lVert Ax \rVert_2}{\lVert x \rVert_2}
    \end{equation}
    wobei $\lVert \cdot \rVert_2: \R^m \to \R$ als Vektornorm die gewöhnliche euklidische Norm ist.
\end{defin}

\begin{lemma}
    Für $A \in \R^{m \times n}$ gilt:
    \begin{equation}
        \lVert A \rVert_2 = \sigma_1
    \end{equation}
\end{lemma}

\begin{proof}
    Sei $S = A^TA$. Es gilt $\lVert Ax \rVert_2^2 = x^TA^TAx$ und $\lVert x \rVert_2^2 = x^Tx$.
    Durch einfache Differentation nach $x_i$ und der Quotientenregel erhält man:
    \begin{equation}
        \partial_{x_i}\frac{x^TSx}{x^Tx} = (x^Tx)^{-2}\left[ (x^Tx) \cdot 2(Sx)_i - 2x_i \cdot (x^TSx) \right]
    \end{equation}

    Setzt man nun die $m$ Ableitungen auf $0$ und formt die Gleichung um erhält man:
    \begin{equation}
        2Sx = \underbrace{\frac{x^TSx}{x^Tx}}_{:= \lambda} 2x = 2\lambda x
    \end{equation}
    also ist $x$ Eigenvektor zum Eigenwert $\lambda$ von $S = A^TA$ und
    somit ist das Maximum von $\frac{\lVert Ax \rVert}{\lVert x \rVert}$ gleich $\sqrt{\lambda_1} = \sigma_1$ wobei $\lambda_1$ der größte Eigenwert von $A^TA$ ist.
\end{proof}

\begin{thm} \textbf{Satz von Eckard-Young für die Spektralnorm}

    Seien $A, B \in \R^{m \times n}$ mit ${\rm Rang}(A) = r$ und ${\rm Rang}(B) \leq k \leq r$. Dann gilt:
    \begin{equation}
        \lVert A - B \rVert_2 \geq \sigma_{k+1}
    \end{equation}
    Im Fall $k = r$ ist $\sigma_{r+1} = 0$.
\end{thm}

\begin{proof}
    Nach Lemma 1 ist $\lVert A - A_k \rVert_2 = \sigma_{k+1}$. Wähle $x \neq 0$ so, dass $Bx = 0$ und $x = \sum\limits_{i=1}^{k+1} c_i^2v_i$.

    Wegen ${\rm dim}(\mathcal{N}(B)) = n - k$ und ${\rm dim}({\rm span}\{v_1, \cdots, v_{k+1}\}) = k + 1$ hat der Schnitt von $\mathcal{N}(B)$ und ${\rm span}\{v_1, \cdots, v_{k+1}\}$ mindestens Dimension $1$. 
    Damit ist sichergestellt das ein $x$ wie oben gewählt, existiert. Nun folgt mit der Wahl von $x$:
    \begin{align*}
        &\lVert(A-B)x\rVert_2^2 = \lVert Ax \rVert^2_2 = \left\lVert \sum\limits_{i=1}^{k+1} c_i\sigma_i u_i \right\rVert^2 = \sum\limits_{i=1}^{k+1}c_i^2\sigma_i^2 \geq \left(\sum\limits_{i=1}^{k+1}c_i^2\right)\sigma_{k+1}^2 = \lVert x \rVert_2^2 \sigma_{k+1}^2 \\
    \Rightarrow \:\:\: &\frac{\lVert (A - B)x \rVert_2}{\lVert x \rVert_2} \geq \sigma_{k+1}
    \end{align*}
\end{proof}

Für die PCA und die PLSR wird die Fourmulierung mit der Frobenius-Norm $\lVert \cdot \rVert_F$ benötigt.

\begin{defin}
    Sei $A \in \R^{m \times n}$. Dann ist die \textbf{Frobenius-Norm} von $A$ definiert durch:
    \begin{equation}
        \lVert A \rVert_F \: := \: \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} |a_{ij}|^2}
    \end{equation}
\end{defin}

\begin{bem}
    Für $A \in \R^{m \times n}$ gilt:
    \begin{equation}
        \lVert A \rVert_F^2 \overset{*}{=} {\rm tr(A^TA)} = (A^TA)_{11} + (A^TA)_{22} + \cdots + (A^TA)_{nn} = \sigma_1^2 + \sigma_2^2 + \cdots + \sigma_n^2        
    \end{equation}
\end{bem}

\begin{proof}
    Die Gleichheit (*) gilt, da für $\forall i\in \{1, \cdots, n\}$ gilt: $(A^TA)_{ii} = a_i^Ta_i = \lVert a_i \rVert^2_2$, wobei $a_i$ die i-te Spalte von A ist.
    Der Rest folgt aus dem Beweis für die SVD.
\end{proof}

\begin{thm} \textbf{Satz von Eckard-Young für die Frobenius-Norm}

    Seien $A, B \in \R^{m \times n}$ mit ${\rm Rang}(A) = r$ und ${\rm Rang}(B) \leq k \leq r$. Dann gilt:
    \begin{equation}
        \lVert A - B \rVert_F \geq \lVert A - A_k \rVert_F
    \end{equation}
\end{thm}

\begin{proof}
    Seien $A,B \in \R^{m \times n}$ mit ${\rm Rang}(B) \leq k \leq {\rm Rang(A)} = r$, sodass für alle $C \in \R^{m \times n}$ gilt: 
    \begin{equation*}
        \lVert A - C \rVert_F \geq \lVert A - B \rVert_F
    \end{equation*}

    Sei nun $B=U D V^T$ die SVD von $B$. Somit ist $D$ eine Diagonalmatrix mit $D \in \R^{k\times k}$. 
    Die orthogonalen Matritzen $U,V$ diagonalisieren $A$ im Allgemeinen nicht. Somit gilt:
    \begin{equation*}
        A = U \begin{bmatrix}
            L+E+R & F\\
            G & H
        \end{bmatrix} V^T
    \end{equation*} 
    Dabei ist $L$ eine untere Dreiecksmatrix, $R$ eine obere Dreiecksmatrix und $E$ eine Diagonalmatrix. Betrachte nun die Matrix $C$ mit ${\rm Rang}(C) \leq k$:
    \begin{equation*}
        C = U \begin{bmatrix}
            L+D+R & F \\
            0 & 0
        \end{bmatrix} V^T
    \end{equation*}
    Nun folgt aus dem Fakt, dass $\lVert \cdot \rVert_F$ invariant bzgl. der Multiplikation mit $U, V^T$ und der Definition von C sofort:
    \begin{equation*}
        \lVert A - B \rVert_F^2 = \lVert A-C \rVert_F^2 + \lVert L \rVert_F^2 + \lVert R \rVert_F^2 + \lVert F \rVert_F^2
    \end{equation*}
    und aus der Minimalitätsbediengung von $\lVert A - B \rVert_F$ folgt $L = R = F = 0$. Genauso kann man beweisen, dass $G = 0$ gilt. Damit kann man A schreiben als:
    \begin{equation}
        A = U \begin{bmatrix}
            E & 0 \\
            0 & H
        \end{bmatrix} V^T \label{eq:SVDA}
    \end{equation}
    Wiederum folgt aus der Minimalitätsbediengung $D = E = {\rm diag}(\sigma_1, \cdots, \sigma_k)$, weil in Gleichung \ref{eq:SVDA} die SVD von A steht.
    Somit gilt für $H$ die Gleichheit $H = {\rm diag}(\sigma_{k+1}, \cdots, \sigma_r)$ und damit:
    \begin{equation*}
        \lVert A - B \rVert_F^2 = \lVert H \rVert_F^2 = \sigma_{k+1}^2 + \cdots + \sigma_{r}^2 = \lVert A - A_k \rVert_F^2 \Rightarrow B = A_k 
    \end{equation*}

\end{proof}

\subsection{Principle Component Analysis (PCA)}
% TODO: include graphic of principle values

Die PCA beruht nun aus den Erkenntnissen des Satzes von Eckard-Young. Gegeben sein eine Matrix $A \in \R^{m\times n}$. 
Diese Matrix ist eine Matrix mit $n$ Messwerten von $m$ Messgrößen.
Im ersten Schritt werden die Daten um den Koordinatenursprung zentriert, indem man von jeder Spalte von $A$ den Mittelwert der Spalte abzieht.
Die PCA führt nun eine Zerlegung der so entstandenen Matrix durch die SVD durch. 
Man erhält nun die Singulärvektoren $u_i$ und die Singulärwerte $\sigma_i$.
Der Satz von Eckard-Young besagt, dass $A$ am besten durch $A_k$ approximiert wird.
Die Daten in $A_k$ sind auf einen $k$-dimensionalen Unterraum projeziert wurden. 
D.h. man hat die Dimension von $m$ auf $k$ reduziert und kann so Rechneraufwand einsparen, 
weil die $m$ ursprünglichen Messgrößen auf $k$ Messgrößen reduziert hat, welche die Daten aber am besten beschreiben.
Was \glqq am besten beschreiben\dq \space bedeutet soll nun erläutert werden:

Die Totale Varianz der Daten $T$ ist gegeben durch $T = \lVert A \rVert_F^2 / (n-1) = (\sigma_1^2 + \cdots + \sigma_r^2) / (n-1)$.
Als Erinnerung: Die Singulärwerte sind der größe nach absteigend sortiert. Somit gilt, dass die Totale Varianz der Daten in Richtung des ersten Singulärvektors $u_1$ maximal ist.
Das kann man so interpretieren, dass sich die Daten im Unterraum, der durch $u_1$ aufgespannt wird, am stärsten voneinander unterscheiden. Sie streuen dort am stärsten.
Den Grund dafür liefert der Satz von Eckard-Young: Er besagt, dass $A_1$ die beste Rang-1-Approximation von $A$ ist. Es folgt: $\lVert A_1 \rVert_F^2 / (n-1) = \sigma_1^2 / (n-1)$.
Diese Argument kann man erweitern auf beliebige $k$. Eckard-Young stellt immer sicher, dass der Unterraum, der durch die Spalten von $A_k$ aufgespannt wird, die Totale Varianz T maximiert 
und in diesem Sinne die Daten am besten beschreibt.

Ein zusätzlicher Effekt ist, dass man durch das Weglassen der Dimensionen mit besonders kleinen Singulärwerten, das Rauschen aus den Daten entfernen kann. 
Das verbessert auch die numerische Stabilität von Analysealgorithmen, welche auf die Daten im Anschluss angewendet werden. 

\subsection{Partial Least Squares Regression (PLSR)}

Die PLSR kann man als eine Verallgemeinerung der PCA verstehen. Es werden nun zwei Datenmatrutzen $X\in \R^{m \times l} ,Y \in \R^{m \times n}$ betrachtet.
Das Ziel ist eine Vorhersage der Daten von $Y$ aus den Informationen aus $X$. 
Anders als bei der PCA wird nun nicht eine Dekomposition von $X^TX$ durchgeführt, um die Varianz der Daten von $X$ zu modellieren, sondern es wird die Kovarinz
zwischen $X$ und $Y$ modelliert, in dem man die Kovarianzmatrix $Y^TX$ zerlegt.

Es folgt eine Herleitung, warum die SVD von $Y^TX$ die Kovarianz korrekt modelliert.

Gegeben seien $v \in \R^m, w \in \R^l$ mit $\lVert v \rVert_2 = \lVert w \rVert_2 = 1$, so dass die Korrelation zwischen $z = v^Tx, r = w^Ty$ maximiert wird.
D.h. es muss gelten:

\begin{equation*}
    \max\limits_{v,w} \E[z \cdot r] = \max\limits_{v,w} \E[v^Tx \cdot w^Ty] = \max\limits_{v,w} v^T\E[x \cdot y]w = \max\limits_{v,w} v^T C_{xy} w, \: (v \in \R^m, w\in\R^l)
\end{equation*}

Das Maximierungsproblem mit Nebenbediengung kann mit Hilfe der Lagrange-Multiplikatorenmethode wie folgt gelöst werden:
\begin{equation*}
    (v^0, w^0) = \arg\max\limits_{v,w} \left\{ \underbrace{v^TC_{xy}w - \frac{1}{2}\lambda_v(v^Tv - 1) - \frac{1}{2}\lambda_w(w^Tw - 1)}_{:=f(v,w)} \right\}
\end{equation*}

\begin{align*}
    \frac{\partial f}{\partial v} &= 0 \Rightarrow C_{xy}w-\lambda_vv = 0 \\
    \frac{\partial f}{\partial v} &= 0 \Rightarrow C_{xy}^Tv - \lambda_ww = 0 \\
    \Rightarrow w &= \frac{1}{\lambda_v}C_{yx}v, (C_{yx}=C_{xy}^T) \\
    \Rightarrow C_{xy}C_{yx}v &= \lambda_v\lambda_w v \Rightarrow v \text{ ist Eigenvektor von } C_{xy}C_{yx} \\
    \Rightarrow C_{yx}C_{xy}w &= \lambda_v\lambda_w w \Rightarrow w \text{ ist Eigenvektor von } C_{yx}C_{xy}
\end{align*}

Diese Herleitung zeigt, dass die Eigenvektoren der entsprechenden Kovarianzmatritzen zu den gewünschten Vektoren führen, die das Maximierungsproblem lösen.
Zusätzlich sind die Eigenwerte von $C_{xy}C_{yx}$ und $C_{yx}C_{xy}$ gleich.

\begin{align*}
    C_{xy}w - \lambda_vv &= 0 \Rightarrow v^TC_{xy}w - \lambda_vvv^T = 0 \Rightarrow \lambda_v = v^TC_{xy}w \\
    C_{yx}v - \lambda_ww &= 0 \Rightarrow w^TC_{yx}v - \lambda_www^T = 0 \Rightarrow \lambda_w = w^TC_{xy}^Tv \\
    \Rightarrow \lambda_v &= v^TC_{xy}w = (w^TC_{xy}^Tv)^T = \lambda_w
\end{align*}

\section{MLPS}
% TODO: example pictures for erosion and dilation and opening

Der letzte Abschnitt beschreibt eine speziellen Algorithmus zur Hintergrundkorrektur von spektralen Daten.
Spektrale Daten sind z.B. physikalische Messwerte aus einem Spektrum beispielsweise dem Lichtspektrum. 
Sie bestehen aus Intensitäten bestimmter Wellenlängen aus dem das gemessene Signal besteht. 
Misst man beispielsweise mit einem Lichtspektrometer, welches die Intensitäten der Wellenlängen einer bestimmten Lichtquelle bestimmt, das Licht einer blauen LED,
so wird man sehen, dass hauptsächlich die Wellenlängen im Signal einen großen Einfluss haben, die blauem Licht zuzuordnen sind. Andere Wellenlängen haben keinen oder nur einen sehr geringen Einfluss.

Messungen dieser Art können durch einen Hintergrund stark verzerrt werden. In der Lichtanalogie wäre der Hintergrund zum Beispiel die Laborbeleuchtung, welche aus weißen besteht. 
Sie würde in der Messung einer bestimmten Lichtquelle alle die Intensitäten aller Wellenlängen erhöhen, sodass die Messung verfälscht wird.
Wenn der Signal-Rauschen-Abstand zu groß wird, dann ist die gesamte Messung wertlos. Deswegen muss man vor der genaueren Analyse solcher Spektraldaten den Hintergrund, welcher aus der Umgebung kommt, entfernen.

Die Arbeit beschreibt einen Algorithmus, welcher Methoden aus der mathematischen Morphologie und Penalized Least Squares verbindet.

\subsection{Mathematische Morphologie}
Die mathematische Morphologie ist eine Technik zur Analyse geometrischer Eigenschaften.
Sie verwendet dafür topologische und megentheoretische Hilfsmittel. Zum Einsatz kommt sie hauptsächlich im Kontext der Analyse von Bildern also 2 dimensionalen Daten.
Die wissenschaftliche Arbeit hat eine Variante entwickelt, diese Technik ebenfalls auf eindimensionale Daten wie zum Beispiel spektrale Messwerte anzuwenden.

Die beiden Hauptoperationen, die in der mathematischen Morphologie genutzt werden, sind die \textbf{Dilatation} und die \textbf{Erosion} zusätzlich zu den megentheoretische Operationen (Vereinigung, Schnitt, Differenz).
Aus diesen können anschließend alle anderen Operationen zusammengestellt werden. Insbesondere die Operation der Öffnung, welche dann im MLPS-Algorithmus verwendet wird.


Bei der beschreibung der folgenden Operationen wird eine Strukturmaske $B \subseteq \R^2$ (meistens eine Kreisfläche) verwendet und welche als Teilmenge des Bildes $A \in \R^2$ aufgefasst wird.
$B_x$ bezeichnet dann die Translation der Strukturmaske zum Punkt $x$. Dafür muss wird ein bestimmter Referenzpunkt $b \in B$ festgelegt, der nach der Translation identisch zum Punkt $x$ sein soll.
Beispielsweise könnte man für die Kreisscheibe den Mittelpunkt als Referenzpunkt wählen, sodass $B_x$ dann die Kreisscheibe mit dem Zentrum $x$ ist.

\subsubsection{Dilatation}
\begin{equation}
    {\rm Dil}(A) := \bigcup\limits_{x \in A} B_x
\end{equation}

Diese Operation transliert die Strukturmaske an jeden Punkt des Bildes $A$ und vereinigt alle Translate $B_x$. Das führt zu einem "Aufblähen" des Originalbilds um die Strukturmaske.

\subsubsection{Erosion}
\begin{equation}
    {\rm Er}(A) := \bigcup \{B_x \: | \: x \in A \land B_x \subseteq A\}
\end{equation}

Diese Operation transliert die Strukturmaske ebenfalls an jeden Punkt des Bildes, vereinigt jedoch nur die Translate, welche sich vollständig im Ursprünglichen Bild befinden.
Das führt zu einer "Verkleinerung" des Bildes. Lange dünne Spitzen, die aus dem Bild herausragen, werden je nach Größe und Form der Strukturmaske abgeschnitten.

\subsubsection{Öffnung}

Die Öffnung ist die Hintereinanderausführung von Erosion und Dilatation, also:
\begin{equation}
    {\rm Op}(A) := ({\rm Dil} \circ {\rm Er})(A) = {\rm Dil}({\rm Er}(A))
\end{equation}

Wie beschrieben entfernt die Erosion Spitzen, die aus dem Bild herausragen und die anschließende Dilatation macht die Errosion an dem verbleibenden Stellen wieder rückgängig.

\subsubsection{Im Kontext von MLPS}

Nun wird nicht mehr mit 2 dimensionalen Bildern als Eingabe gearbeitet sondern mit eindimensionalen Messwerten. Die Daten werden in einem $(x,y)$-Koordinatensystem abgetragen. 
Sei nun $(x,f(x)) \in \R^2$ ein Messwert. Somit sei $f:\R \to \R$ die Funktion, die für alle $x \in \R$ den gemessenen Wert ausgibt. Als eindimensionales Analogon zur Kreisscheibe wird nun ein Intervall verwendet. 
Dieses Fenster sei $W_x := [x-w, x+w]$ mit $w \in \R_+$. Die halbe Fensterbreite $w$ ist dabei ein Hyperparameter, der manuell festgelegt werden muss. 
Dieses Fenster wird nun so transliert, dass sein Zentrum mit $x$ übereinstimmt.

Für die Dilatation wird dann das Maximum über die im Fenster befindlichen Messwerte genommen und für die Erosion jeweils das Minimum. Somit gilt:

\begin{align}
    {\rm Er}(x)  &= \min\limits_{\tilde{x} \in W_x} f(\tilde{x}) \\
    {\rm Dil}(x) &= \max\limits_{\tilde{x} \in W_x} f(\tilde{x})
\end{align}

Die Definition def Öffnung bleibt gleich, wie im 2 dimensionalen Fall.
Für echte Daten muss dieses Definition noch diskretisiert werden, weil die Messwerte nicht durch eine Funktion $f$ die auf ganz $\R$ existiert, definiert sind, sondern eine diskrete Menge von Punktion $X := \{(x_i,y_i) \in \R^2 \: | \: n\in \mathbb{N}, i=1,\cdots,n \}$.

Die beiden Operatoren ändern sich für jedes $i\in \{1,\cdots,n\}$ zu:
\begin{align}
    {\rm Er}(x_i) &= \min \{y_{i+j} \: | \: j=-w,\cdots,w \: \land \: 0 \leq i + j \leq n\} \\
    {\rm Dil}(x_i)  &= \max \{y_{i+j} \: | \: j=-w,\cdots,w \: \land \: 0 \leq i + j \leq n\}
\end{align}

Dabei ist $w \in \mathbb{N}$ nun eine diskrete Fenstergröße.

\subsection{Penalized Least Squares}
Penalized Least Squared ist eine Variante der Least Squares Optimierung.

\begin{align}
    Ax = b \label{eq:ls} \\ 
    \min\limits_{x \in \R^n} \lVert b - Ax \rVert \label{eq:minls}
\end{align}
$x \in \R^n, b \in \R^m, A \in \R^{m\times n}$.

Das gewöhnliche Least-Squares-Verfahren wird verwendet, um ein lineares Gleichugssystem zu lösen. 
Dieses kann als Matrix-Vektor-Produkt dargestellt werden wie in Gleichung \ref{eq:ls}. 
Aus dieser kann man das Minimierungsproblem herleiten, welches in Gleichung \ref{eq:minls}.
Dabei sei erwähnt, dass die Matrix $A$ nicht invertierbar sein muss und sich $b$ auch nicht im Spaltenraum von $A$ befinden muss.
Das führt dazu, dass es keine eindeutige beste Approximation einer Lösung geben muss.
Least Squares liefert jedoch immer eine eindeutige Lösung der Form $\hat{x} = A^{\dagger} b$, wobei $A^{\dagger}$ die Moore-Pensrose-Pseudoinverse ist.
Diese ist die eindeutig bestimmte Lösung, die \ref{eq:minls} erfüllt und von allen Lösungen, die ebenfalls \ref{eq:minls} erfüllen, besitzt $\hat{x}$ die kleinste Norm.

Penalized-Least-Squares erweitert das Optimierungsproblem aus \ref{eq:minls} nun um ein Regularisierungsfunktional der Form $R: \R^n \to \R$.
Die neue Optimierungsproblem hat dann die Form:

\begin{equation}
    \min\limits_{x \in \R^n} \lVert b - Ax \rVert + \lambda R(x)
\end{equation}
wobei $\lambda \in \R$ wieder ein Hyperparameter ist, welcher den Einfluss der Regularisierungen steuert.

Spezielle zu erwähnen sind die LASSO und $\ell^2$-Regularisierungen. Für LASSO gilt: $R: \R^n \ni x \mapsto \lVert x \rVert_1$ und für die $\ell^2$-Regularisierung gilt: $R: \R^n \ni x \mapsto \lVert x \rVert_2$.
LASSO liefert dabei dünnbesetzte Lösungen mit vielen Nulleinträgen. 
$\ell^2$ hält die Norm der Lösung beschränkt, sodass es keine besonders großen Werte in $\hat{x}$ gibt.

\subsection{Signal Hintergrundkorrektur}

Im folgenden Abschnitt bezeichnet gilt: $x \in R^n$ ist der Vektor der Messwerte eines Signals. 

Als erstes führt der Algorithmus eine diskrete morphologische Öffnung mit den Messwerten $x$ durch. Dabei muss eine Fenstergröße $w$ für den Algorithmus festgelegt werden.
In der wissenschaftlichen Arbeit wurde mit den verschiedenen Fenstergrößen ($w=11,13,15$) experiementiert, welche alle zu einem sehr ähnlichen Ergebnis führten.
Die Öffnung bringt nun eine grobe erste Schätzung für das Hintergrundspektrum, welches entfernt werden soll.
An die Peaks der Messung kann man jedoch erkenne, dass sich durch die Öffnung Plateaus formen, sodass das Hintergrundspektrum dort nur sehr schlecht approximiert ist.

Deswegen wird nun zusätzlich eine Penalized-Least-Square-Regression mit den lokalen Minima um die Peaks durchgeführt.
Diese Minima bestimmt man, indem man die als erstes die Start und Endpunkte dieser Plateaus bei den Peaks bestimmt.
Sei $s_k \in \N$ der Index des Startpunkts und $e_k \in \N$ der Index des Endpunktes des Plateaus am $k$-ten Peaks. Das $k$-te Minimum ist nun definiert durch:
\begin{equation}
    M_k = \arg\min \{x_j \: | \: j=e_k,\cdots s_{k+1}\}
\end{equation}

MLPS nutzt die Rauhigkeit der Least-Squares-Lösung als Regularisierungsfunktional.
\begin{equation}
    R: \R^n \ni x \mapsto \sum_{i=2}^{n} (x_i - x_{i-1})^2 = \lVert Dx \rVert_2
\end{equation}
wobei $D \in \R^{n\times n}$ definiert ist durch $(d_{ij})_{i=1,j=1}^{n-1,n} = (\delta_{i,j-1} - \delta_{i,j})_{i=1,j=1}^{n-1,n}$, wobei $\delta_{i,j}$ das Kronecker-Delta ist.

Die Lösung dieses neuen Problems ist dann gegeben durch:
\begin{equation}
    \hat{x} = (W + \lambda D^TD)^{-1}Wb
\end{equation}

Die Lösung $\hat{x}$ ist nun das approximierte Hintergrundspektrum.

Man kann nun erkennen, dass nach der Regression die Plateaus verschwunden sind und der Hintergrund besser approximiert wird also durch die isolierte Anwendung der morphologischen Methoden. 

\end{document}
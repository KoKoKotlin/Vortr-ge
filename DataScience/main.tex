\documentclass{article}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\setlength{\parindent}{0mm}

\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\F}[0]{\mathcal{F}}

\title{Datascience}
\author{Yannik Höll}
\date{\today}

\newtheorem{defin}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{definthm}{Definition \& Satz}
\newtheorem{thm}{Satz}

\begin{document}
\maketitle

\section{Inverse Faltung}

\subsection{Klassische Fouriertransformation und Faltung}

Für folgende Definitionen sei $n \in \mathbb{N} \setminus \{ 0 \}$.

\begin{defin}
    Seien $f,g \in L^1(\R^n)$. \\
    Dann ist die \textbf{Konvolution} von $f$ mit $g$ für $x \in \Omega$ definiert als:
    \begin{equation}
        (f \star g)(x) := \int_{\R^n} f(x - y) g(y) dy
    \end{equation}
\end{defin}

\begin{defin} Die \textbf{Fourier-Transformation} einer Funktion $f \in L^1(\R^n)$ ist definiert durch:
    \begin{equation}
        (\mathcal{F}f)(\xi) := \int_{\R^n} f(x) \cdot e^{-2\pi i x \cdot \xi} dx
    \end{equation}
    für $x, \xi \in \R^n$. Dabei ist $x \cdot \xi$ das Skalarprodukt im $\R^n$.
\end{defin}

Die Fouriertransformation ist wohldefiniert, weil $f \in L^1(\R^n)$.
\begin{proof}
    Sei $f \in L^1(\R^n)$, dann gilt:
    \begin{align*}
        \Bigg\lvert \int_{\R^n} f(x) \cdot e^{-2\pi i x \cdot \xi} dx \Bigg\rvert &\leq \int_{\R^n} \lvert f(x) \rvert \lvert e^{-2\pi i x \cdot \xi} \rvert dx \\
        &= \int_{\R^n} \lvert f(x) \rvert dx \\
        &= \lVert f \rVert_{L^1(\R^n)} < \infty
    \end{align*}
\end{proof}

\begin{defin}
    Die \textbf{Inverse Fouriertransformation} einer Funktion $f \in L^1(\R^n)$ ist definiert durch:
    \begin{equation}
        (\mathcal{F}f)(\xi) := \int_{\R^n} f(x) \cdot e^{2\pi i x \cdot \xi} dx
    \end{equation}
    für $x, \xi \in \R^n$. Dabei ist $x \cdot \xi$ das Skalarprodukt im $\R^n$.
\end{defin}
Der Nachweis der Wohldefiniertheit funktioniert genauso, wie bei der Fouriertransformation.

\begin{thm}
    \textbf{Faltungssatz} \\
    Seien $f,g \in L^1(\R^n)$ und $\mathcal{F}f \cdot \mathcal{F}g \in L^1(\R^n)$. Dann gilt:
    \begin{equation}
        \mathcal{F}(f \star g) = \mathcal{F}f \cdot \mathcal{F}g
    \end{equation}

    \begin{proof}
        Der Beweis ist abgeschlossen, wenn gezeigt wurde, dass $\F^{-1} (\F f \cdot \F g) = f \star g$. 
        
        Sei $F(\omega) := \F f(\omega)$ und $G(\omega) := \F g(\omega)$. Dann gilt: \\
        \begin{align*}
            \F^{-1} (F(\omega) G(\omega)) &= \int_{\R^n} F(\omega) G(\omega) \cdot e^{2\pi i \omega \cdot t} d\omega \\
            &= \int_{\R^n} \Bigg\lbrack \int_{\R^n} f(s) \cdot e^{-2\pi i s \cdot \omega} ds \Bigg\rbrack G(\omega) \cdot e^{2\pi i \omega \cdot} d\omega \\
            &= \int_{\R^n} \int_{\R^n} f(s) G(\omega) \cdot e^{2\pi i \omega \cdot (s - t)} ds d\omega
        \end{align*}
        Nach dem Satz von Fubini kann die Integrationsreihenfolge vertauscht werden:
        \begin{align*}
            \F^{-1} (F(\omega) G(\omega)) &= \int_{\R^n} \int_{\R^n} f(s) G(\omega) \cdot e^{2\pi i \omega \cdot (s - t)} d\omega ds \\
            &= \int_{\R^n} f(s) \Bigg \lbrack \int_{\R^n} G(\omega) \cdot e^{2\pi i \omega \cdot (t - s)} d\omega \Bigg \rbrack ds \\
            &= \int_{\R^n} f(s) g(t-s) ds = f \star g
        \end{align*}
    \end{proof}
\end{thm}

\subsection{Diskrete Fouriertransformation}

Mit Satz 1 ist es nun theoretisch möglich, die Faltung von zwei Funktionen umzukehren, unter der Vorraussetzung, 
dass eine der beiden Funktionen bekannt ist. Dafür kann man die Fouriertransformation wie folgt nutzen:

Seien $w: \R \to \R$ und $f, g \in L^1(\R)$, sodass gilt: $w = f \star g$. Dabei sind $w$ und $g$ bekannt und $f$ ist die gesuchte Funktion.
Wenn $w$ und $g$ hinreichund gute Eigenschaften haben, sodass die Integrale existieren und die Inverse Fouriertransformation existiert, dann kann
man folgenden Algorithmus nutzen, um $f$ zu berechnen:
\begin{enumerate}
    \item Berechne $\hat{w} := \mathcal{F}w$ und $\hat{g} := \mathcal{F}g$.
    \item Berechne $\hat{f} := \frac{\hat{w}}{\hat{g}}$.
    \item Berechne $f := \mathcal{F}^{-1}\hat{f}$.
\end{enumerate}

Durch die Berechnung im 2. Schritt wird sofort deutlich, dass $\hat{g}$ keine Nullstellen besitzen darf,
da ansonsten der Quotient nicht existiert. Zusätzlich muss man auch über Stetigkeitseigenschaften der Methode nachdenken,
da man Komponenten von hohen Frequenzen von $w$ potentiell stark aufgebläht werden 
und damit auch potentielle Messfehler verstärt werden können.

Für konkrete Berechnungen benötigt man die diskrete Fouriertransformation. 
Sei dafür $u \in C^0(\R)$ mit $supp(u) := \{x \in \R \: | \: u(x) \neq 0 \} \subseteq [-a, a], a > 0$. Diese Funktion wird an $N \in \mathbb{N}_{>0}$ äquidistanten Stellen betrachtet, 
die z.B. Messwerte darstellen können. Sei dafür $u_j = u(t_j), t_j := jh, h:= \frac{2a}{N}, j \in \{-\frac{N}{2}, \cdots, \frac{N}{2} - 1\}$.

Dann kann $u$ durch lineare B-Splines approximiert werden durch:
\begin{equation}
    B_2(t) := \begin{cases}
        t + 1, &-1 \leq t \leq 0, \\
        1 - t, &0 \leq t \leq 1, \\
        0, &{\rm sonst}.
    \end{cases} \\
\end{equation}
\begin{equation}
    u_N(t) := \sum\limits_{j=-N/2}^{N/2-1} u_j B_2 \left(\frac{t}{h} - j\right)
\end{equation}

Durch Fouriertransformation erhält man:
\begin{equation}
    \mathcal{F}(u_N)(y) := \left(\frac{{\rm sin}(\pi h y)}{\pi h y}\right) \cdot \underbrace{\left(h \sum\limits_{j=-N/2}^{N/2-1} u_j e^{-2\pi i jhy}\right)}_{:=U(y)}
\end{equation}

Wählt man $y := \frac{k}{2a}, \: k \in \mathbb{Z}$, dann sind die Stellen, an denen man die Funktion $u$ und die Transformierte $\mathcal{F}(u_N)$
auswertet äquidistant. Damit erhält man schlussendlich:

\begin{align}
    U_k = \frac{1}{N} \sum\limits_{j=-N/2}^{N/2-1} u_j e^{-2\pi ijk/N}
\iff u_j = \frac{1}{N} \sum\limits_{k=-N/2}^{N/2-1} U_k e^{2k\pi ijk/N}
\end{align}

Dabei sind die $U_k$ nun die diskreten Fourierkoeffizienten von $u$ zu den entsprechenden Stützstellen.

Diese diskrete Variante der Transformation kann nun verwendet werden, um die Inverse Faltung wie oben beschrieben für konkrete Messwerte zu berechnen. 
Es handelt sich allerdings um ein schlecht gestelltes Problem und man muss Regularisierungen anwenden, um die die schlechten Stetigkeitseigenschaften der Methode zu verbessern. 

\section{PLSR}

%TODO: maybe m and n have to be swapped

\subsection{Singulärwert-Zerlegung (SVD)}
Die Singulärwertzerlegung ist eine Verallgemeinerung der Eigenwertzerlegung einer Matrix, die auch auf nicht-quadratische, singuläre Matritzen angewandt werden kann.

\begin{definthm}
    Sei $A \in \R^{n \times m}$ eine reellwertige Matrix mit dem Rang $r$ $(n, m \in \mathbb{N}_{>0})$.

    Die \textbf{Singulärwertzerlegung} von $A$  hat folgende Gestalt:
    \begin{equation}
        A = U \Sigma V^T
    \end{equation}
    wobei $U \in \R^{n \times r}, \; \Sigma \in \R^{r \times r}, \; V \in \R^{m \times r}$.
    Die Spalten von $U$ nennt man die \textbf{Links-Singulärvektoren} und die von $V$ die \textbf{Rechts-Singulärvektoren} von $A$. $U, V$ sind orthogonale Matritzen.
    $\Sigma$ ist eine Diagonalmatrix deren Einträge $\sigma_i$ $(i \in \{1, \cdots, r\})$ die \textbf{Singulärwerte} von $A$ genannt werden.
    
    Die Matritzen erfüllen folgendes Gleichungssystem:
    \begin{align}
        &Mv_i = \sigma_i u_i, \:&&\forall i \in \{1, \cdots, r\} \\
        &Mv_i = 0, \: &&\forall i \in \{r, \cdots, m\}
    \end{align}

    Diese Zerlegung existiert für jede beliebige Matrix $A \in \R^{n \times m}$.
\end{definthm}

\begin{proof}
    Betrachte
    \begin{align}
        A^TA = (V \Sigma^T U^T) (U \Sigma V^T) &= V \Sigma^T \Sigma V^T \\
        AA^T = (U \Sigma V^T) (V \Sigma^T U^T) &= U \Sigma \Sigma^T U^T
    \end{align}
    Diese beiden Matritzen sind symmetrische, quadratische Matritzen und haben somit
    nicht-negative Eigenwerte. Man kann also beide Matritzen in der Form $Q \Lambda Q^T$ darstellen.
    Dabei ist $V = Q$ für Gleichung (12) und $U = Q$ für Gleichung (13). Und somit sind $V$ bzw. $U$ die Eigenwerte
    von $A^TA$ bzw. $AA^T$ und insbesondere orthogonale Matritzen.

    \textbf{Konstruktion von $V$}: Wähle orthonormale Eigenvektoren $v_1, \cdots, v_r$ von $A^TA$.

    \textbf{Konstruktion von $\Sigma$}: Setze $\sigma_i = \sqrt{\lambda_i}, \: \forall i \in \{1, \cdots, r\}, \lambda_i \in \Lambda$.

    \textbf{Konstruktion von $U$}: Setze $u_i = \frac{Av_i}{\sigma_i}, \: \forall i \in \{1, \cdots, r\}$.
    
    \textbf{Behautung}: 
    \begin{enumerate}
        \item $u_i$ sind die Eigenvektoren zu den Eigenwerten $\lambda_i$ für $AA^T$.
        \item Die $u_i$ sind orthonormale Vektoren.
    \end{enumerate}
    \begin{equation*}
        AA^Tu_k = AA^T(\frac{Av_k}{\sigma_k}) = A\left(\frac{A^TAv_k}{\sigma_k}\right) = A\frac{\sigma_k^2v_k}{\sigma_k} = \sigma_k^2u_k
    \end{equation*}
    Somit ist 1. von der Behauptung gezeigt.

    \begin{equation*}
        u_j^Tu_k = \left(\frac{Av_j}{\sigma_j}\right)^T\left(\frac{Av_k}{\sigma_k}\right) = \frac{v_j^T(A^TAv_k)}{\sigma_j\sigma_k} = \frac{\sigma_k}{\sigma_j}v_j^Tv_k = \begin{cases}
            1, &\text{für } j = k,\\
            0, &\text{sonst}.
        \end{cases}
    \end{equation*}
    Das zeigt den 2. Teil der Behauptung.

    Nun müssen noch die letzen $m - r$ Vektoren für $V$ und die letzen $n - r$ Vektoren für $U$ gewählt werden.
    Man wählt eine Orthonormalbasis für den Nullraum von $A$ und diese bilden dann die fehlenden Vektoren für $V$ 
    und weiterhin wählt man eine Orthonormalbasis für den Nullraum von $A^T$ und diese bilden die fehlenden Vektoren für $U$.
    Die so gewählten Vektoren $v_i$ sind automatisch orthogonal zu den bereits gewählten $v_1, \cdots, v_r$ 
    und dasselbe gilt für die so gewählten $u_i$. 
\end{proof}

\begin{thm} \textbf{Satz von Eckard-Young}
    
    Sei $A \in \R^{n\times m}$ eine Rang $r$ Matrix. Weiterhin sei $A_k = \sum\limits_{i=1}^k u_i\sigma_iv_i^T$. 
    Wobei $\sigma_i$ die ersten $k$ Singulärwerte von A sind, wobei gilt $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r \geq 0$. Die $u_i, v_i$ sind die dazugehörigen Singulärvektoren. 
    Dann gilt für $\forall B \in \R^{n \times m}$ mit Rang $r$:
    \begin{equation}
        \lVert A - B \rVert \geq \lVert A - A_k \rVert
    \end{equation}
\end{thm}

Diese Formulierung gilt für eine ganze Reihe von verschiedenen Operatornormen.
Für die PCA und die PLSR wird die Fourmulierung mit der Spektralnorm $\lVert \cdot \rVert_2$ benötigt.

\begin{defin}
    Sei $A \in R^{n \times m}$. Dann ist die \textbf{Spektralnorm} von $A$ definiert durch:
    \begin{equation}
        \lVert A \rVert_2 \: := \: \max\limits_{x \neq 0} \frac{\lVert Ax \rVert_2}{\lVert x \rVert_2}
    \end{equation}
    wobei $\lVert \cdot \rVert_2: \R^m \to \R$ als Vektornorm die gewöhnliche euklidische Norm ist.
\end{defin}

\begin{lemma}
    Für $A \in \R^{n \times m}$ gilt:
    \begin{equation}
        \lVert A \rVert_2 = \sigma_1
    \end{equation}
\end{lemma}

\begin{proof}
    Sei $S = A^TA$. Es gilt $\lVert Ax \rVert_2^2 = x^TA^TAx$ und $\lVert x \rVert_2^2 = x^Tx$.
    Durch einfache Differentation nach $x_i$ und der Quotientenregel erhält man:
    \begin{equation}
        \partial_{x_i}\frac{x^TSx}{x^Tx} = (x^Tx)^{-2}\left[ (x^Tx) \cdot 2(Sx)_i - 2x_i \cdot (x^TSx) \right]
    \end{equation}

    Setzt man nun die $m$ Ableitungen auf $0$ und formt die Gleichung um erhält man:
    \begin{equation}
        2Sx = \underbrace{\frac{x^TSx}{x^Tx}}_{:= \lambda} 2x = 2\lambda x
    \end{equation}
    also ist $x$ Eigenvektor zum Eigenwert $\lambda$ von $S = A^TA$ und
    somit ist das Maximum von $\frac{\lVert Ax \rVert}{\lVert x \rVert}$ gleich $\sqrt{\lambda_1} = \sigma_1$ wobei $\lambda_1$ der größte Eigenwert von $A^TA$ ist.
\end{proof}

\begin{thm} \textbf{Satz von Eckard-Young für die Spektralnorm}

    Seien $A, B \in \R^{n \times n}$ mit ${\rm Rang}(A) = r$ und ${\rm Rang}(B) \leq k \leq r$. Dann gilt:
    \begin{equation}
        \lVert A - B \rVert_2 \geq \sigma_{k+1}
    \end{equation}
\end{thm}

\begin{proof}
    Nach Lemma 1 ist $\lVert A - A_k \rVert_2 = \sigma_{k+1}$. Wähle $x \neq 0$ so, dass $Bx = 0$ und $x = \sum\limits_{i=1}^{k+1} c_i^2v_i$.

    Wegen ${\rm dim}({\rm Kern}(B)) = n - k$ und ${\rm dim}({\rm span}\{v_1, \cdots, v_{k+1}\}) = k + 1$ hat der Schnitt von ${\rm Kern}(B)$ und ${\rm span}\{v_1, \cdots, v_{k+1}\}$ mindestens Dimension $1$. 
    Damit ist sichergestellt das ein $x$ wie oben gewählt, existiert. Nun folgt mit der Wahl von $x$:
    \begin{align*}
        &\lVert(A-B)x\rVert_2^2 = \lVert Ax \rVert^2_2 = \left\lVert \sum\limits_{i=1}^{k+1} c_i\sigma_i u_i \right\rVert^2 = \sum\limits_{i=1}^{k+1}c_i^2\sigma_i^2 \geq \left(\sum\limits_{i=1}^{k+1}c_i^2\right)\sigma_{k+1}^2 = \lVert x \rVert_2^2 \sigma_{k+1}^2 \\
    \Rightarrow \:\:\: &\frac{\lVert (A - B)x \rVert_2}{\lVert x \rVert_2} \geq \sigma_{k+1}
    \end{align*}
\end{proof}

\subsection{Principle Component Analysis (PCA)}


\end{document}